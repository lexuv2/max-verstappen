{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WxO3Mpli3WX"
      },
      "source": [
        "### MountainCar-v0 with DQL\n",
        "Based on [this tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) from the PyTorch website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tah99hAii3WZ",
        "outputId": "a72c46dc-7157-4e72-ea10-4fa8c821f952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "game_name = \"MountainCar-v0\"\n",
        "env = gym.make(game_name)\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "6xeOQVBFi3Wa"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity: int):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args: any) -> None:\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size: int) -> Transition:\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "E7FXGLudi3Wb"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations: int, n_actions: int):\n",
        "        super(DQN, self).__init__()\n",
        "        hidden_size = 64\n",
        "        self.layer1 = nn.Linear(n_observations, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, n_actions)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3yOTmHTi3Wb"
      },
      "source": [
        "#### Hyperparameters and utilities\n",
        "\n",
        "- BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "- GAMMA is the discount factor as mentioned in the previous section\n",
        "- EPS_START is the starting value of epsilon\n",
        "- EPS_END is the final value of epsilon\n",
        "- EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "- TAU is the update rate of the target network\n",
        "- LR is the learning rate of the ``AdamW`` optimizer\n",
        "- MEMORY_SIZE is the maximum size of the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "OKAr_70Li3Wc"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "MEMORY_SIZE = 10000\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(MEMORY_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PIPtw1g5i3Wd"
      },
      "outputs": [],
      "source": [
        "steps_done = 0\n",
        "episode_durations = []\n",
        "\n",
        "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # Pick action with the largest expected reward\n",
        "            return policy_net(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        # Pick a random action\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "def plot_duration(show_result: bool = False) -> None:\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG-SLvdPi3Wd"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_SZWrfi3i3We"
      },
      "outputs": [],
      "source": [
        "def optimize_model() -> None:\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions)) # [(state, action, next_state, reward), ...] -> [(state, ...), (action, ...), (next_state, ...), (reward, ...)]\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state is the one after which the episode ends)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\n",
        "    # These are the actions which would've been taken for each batch state according to policy_net.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based on the \"older\" target_net; selecting their best reward with max(1).values.\n",
        "    # This is merged based on the mask, such that we'll have either the expected state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "0FqOrQNyi3Wf",
        "outputId": "d59c2ee3-2fa9-47be-9fa9-b07b41fe0490"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[88], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the target network)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     39\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
            "Cell \u001b[0;32mIn[87], line 15\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     14\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n\u001b[0;32m---> 15\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# These are the actions which would've been taken for each batch state according to policy_net.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m policy_net(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 100000\n",
        "else:\n",
        "    num_episodes = 10000\n",
        "use_rewards = True\n",
        "\n",
        "record_episode = -1\n",
        "record_policy = None\n",
        "record_duration = 1e9\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    state, info = env.reset(seed=i_episode)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "            if t < record_duration:\n",
        "                record_episode = i_episode\n",
        "                record_duration = t\n",
        "                record_policy = policy_net.state_dict()\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_duration()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "print(f'Best episode: {record_episode}, duration: {record_duration}')\n",
        "plot_duration(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCxLo7QukJZx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Terminated: False, Truncated: True, Steps: 199\n",
            "Replaying the best episode (328): Terminated: False, Truncated: True, Steps: 199\n"
          ]
        }
      ],
      "source": [
        "gui = True\n",
        "env_gui = gym.make(game_name, render_mode='human' if gui else None)\n",
        "\n",
        "def play_game(seed: int = None) -> None:\n",
        "    state, _ = env_gui.reset(seed=seed)\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        observation, _, terminated, truncated, _ = env_gui.step(action.item())\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    print(f'Terminated: {terminated}, Truncated: {truncated}, Steps: {t}')\n",
        "    if gui:\n",
        "        input('Press Enter to continue...')\n",
        "\n",
        "play_game()\n",
        "\n",
        "if record_policy is not None:\n",
        "    print(f'Replaying the best episode ({record_episode}):', end=' ')\n",
        "    backup_policy = policy_net.state_dict()\n",
        "    policy_net.load_state_dict(record_policy)\n",
        "    play_game(seed=record_episode)\n",
        "    policy_net.load_state_dict(backup_policy)\n",
        "\n",
        "env.close()\n",
        "env_gui.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
